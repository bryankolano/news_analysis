{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Political News Stories Collection\n",
    "## A script to gather political stories from CNN.com and foxnews.com\n",
    "## By: Bryan Kolano, December 7th, 2022\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background\n",
    "I was trying to think of a new project that involved text and classification.  After thinking for a while, I got an idea about how doing analysis of political stories might be an interest topic to tackle.  With CNN and Fox News, the language they use is very different, they focus on different topics, and their coverages of the same topics are typically very different.  <br>\n",
    "\n",
    "The point of this script to is grab new headlines each day from each news sources and then combine them in a CSV.  After a couple hundred (maybe a couple thousands) stories are collected, then I plan to do analysis in a different script.  I want to examine differences between the difference sources, and I also plan to test various machine learn algorithms to see if they can correctly classify whether a text comes from CNN or Fox News."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages for scraping and driver\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "#baseline python packages needed\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "#Pandas for data collection/ indexing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collection of CNN Articles\n",
    "\n",
    "From 2019-2021, while I was part of Intelligence and Security Command (INSCOM) Data Science Team, we used to teach a class called \"data Analytics in R\" (called OS305) to open source intelligence analysts.  These analysts were soldiers, army civilians, and government contractors who were looking to be able to do data analysis on data collected from the open internet. <br>\n",
    "\n",
    "These student had minimal experience in R; they had only taken a small coding bootcamp before taking OS305.  As part of OS305, I gave a block of instruction on webscraping.  For the class, we scraped a couple of pre-determined websites to show a website's HTML and CSS and then scrape the data. <br>\n",
    "\n",
    "During one iteration of the class, I decided to call an audible and grab a random website to scrape.  I broke a cardinal rule of teaching coding: don't live code in front of students, haha.  I chose CNN.com and tried to webscrape it using R.  No matter how many ways I tried to manipulate the HTML and CSS structure, I could not pull the information I wanted.  I told the class that I would look into a get back to them.  <br>\n",
    "\n",
    "As it turns out, CNN among many other websites uses JavaScript to render content once they are loaded in the browser.  In other words, in webscraping, you're trying to grab information that doesn't exist yet because you are making a GET request before the content is loaded in the browser.  At the time, I did not know CNN did that, and alas, webscraping would not work for CNN and I shared with the class the reason.\n",
    "<br>\n",
    "\n",
    "Due to the way CNN renders content, it is necessary to use webdriving to navigate CNN and gather data.  Therefore, the following section using Selenium to gather the page information and then use Beautiful Soup to parse the HTML document.  <br>\n",
    "\n",
    "After grabbing the information from each news article, I turn it into a Pandas Dataframe and then write the results to a CSV called \"news_articles.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define CNN urls \n",
    "cnn_base_url = 'https://www.cnn.com'\n",
    "cnn_politics_url = 'https://www.cnn.com/politics'\n",
    "\n",
    "#create Selenium driver element\n",
    "options = Options()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--ignore-ssl-errors')\n",
    "options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "service = Service(executable_path='D:\\Projects\\gun_violence\\chromedriver.exe')\n",
    "driver = webdriver.Chrome(service=service, options= options)\n",
    "\n",
    "try:\n",
    "    #go to the politics page of CNN\n",
    "    driver.get(cnn_politics_url)\n",
    "\n",
    "    #collect HTML of driver and turn into BS element\n",
    "    soup = BeautifulSoup(driver.page_source,'html.parser' )\n",
    "\n",
    "    #grab the html of titles of all articles on the page\n",
    "    politic_titles = soup.select('.container__headline')\n",
    "\n",
    "    #create list of all titles\n",
    "    cnn_titles = [title.text.strip() for title in politic_titles]\n",
    "\n",
    "    #create list of all titles' URLs extensions\n",
    "    url_extensions = [url['href'] for item in soup.select('div.container_lead-plus-headlines__field-links') for url in item.select('a') ]\n",
    "\n",
    "    #combine base URL with each URL extension\n",
    "    cnn_urls = [cnn_base_url + url for url in url_extensions]\n",
    "\n",
    "    if len(cnn_urls) == 0:\n",
    "        raise ValueError('Need to recheck the CNN code, not pulling URLs')\n",
    "\n",
    "    #set up blank lists to append to\n",
    "    cnn_text_of_articles = []\n",
    "    cnn_date = []\n",
    "\n",
    "except WebDriverException:\n",
    "    print('Error getting main page')\n",
    "\n",
    "#loops across all URLs on the politics page\n",
    "for url in cnn_urls:\n",
    "    \n",
    "    try:\n",
    "        #tell driver to grab each url\n",
    "        driver.get(url)\n",
    "\n",
    "        #turn each page into BS element and grab HTML\n",
    "        page_soup = BeautifulSoup(driver.page_source,'html.parser' )\n",
    "        \n",
    "        #find HTML section that contains the text of the article\n",
    "        article_contents =  page_soup.select('body > div.layout__content-wrapper.layout-with-rail__content-wrapper > section.layout__wrapper.layout-with-rail__wrapper > section.layout__main-wrapper.layout-with-rail__main-wrapper > section.layout__main.layout-with-rail__main > article > section > main > div.article__content-container > div.article__content > p')\n",
    "\n",
    "        #take all the <p> of the article sections and join them together\n",
    "        article_text = ' '.join([x.text.strip() for x in article_contents]) \n",
    "\n",
    "        #append article text to our holder list\n",
    "        cnn_text_of_articles.append(article_text)\n",
    "\n",
    "        #Need to grab the date from the article\n",
    "        #find the line in the page HTML that has the date\n",
    "        date_line = page_soup.select('div.timestamp')[0].text.strip()\n",
    "        \n",
    "        #create regex object to rip out the date\n",
    "        date_re = re.compile(r'\\w{3,}\\s\\d{1,2},\\s\\d{4}')\n",
    "        #find the date from the pattern\n",
    "        date = date_re.findall(date_line)[0]\n",
    "        #turn into datetime object\n",
    "        date = datetime.strptime(date, '%B %d, %Y')\n",
    "        #return the date as a string in the format MM/DD/YYYY\n",
    "        current_date = f\"{date.month}/{date.day}/{date.year}\"\n",
    "        #append the current date to the date holder list\n",
    "        cnn_date.append(current_date)\n",
    "\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "driver.close()\n",
    "\n",
    "#create dataframe with all our of filled lists\n",
    "cnn_df = pd.DataFrame(list(zip(cnn_titles, cnn_date, cnn_urls, cnn_text_of_articles)), columns = ['title','date','url', 'article_text'])\n",
    "\n",
    "#creat new column in dataframe with the source of the articles\n",
    "cnn_df['source'] = 'CNN'\n",
    "\n",
    "#write the dataframe to a CSV\n",
    "cnn_df.to_csv('news_articles.csv', index = False, header = False, mode= 'a')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collection of Fox News Articles\n",
    "Fortunately, Fox news does not render its content in the same way CNN does.  Therefore, a standard webscrape with the requests can be used to grab the HTML.  Webdriving is unecessary for grab Fox News data, so I can simply make the GET request and then parse the HTML response with Beautiful Soup.\n",
    "\n",
    "After grabbing all article information, I turn it into a Pandas Dataframe and then write to the same CSV I am adding all the CNN articles to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab base FOX URLs\n",
    "fox_base_url = 'https://www.foxnews.com'\n",
    "fox_politics_url = 'https://www.foxnews.com/politics'\n",
    "\n",
    "#make GET request to Fox New's politics page and grab the HTML\n",
    "resp = requests.get(fox_politics_url).text\n",
    "\n",
    "#turn HTML into Beautiful Soup Object\n",
    "fox_soup = BeautifulSoup(resp, 'html.parser')\n",
    "\n",
    "#set up blank holder lists\n",
    "fox_urls = []\n",
    "fox_titles = []\n",
    "fox_text_of_articles = []\n",
    "fox_date = []\n",
    "\n",
    "#loop across all articles on the politics home page\n",
    "for article in fox_soup.select('main.main-content .content .article'):\n",
    "    \n",
    "    #a few of the elements in this particular CSS selector cause errors, so errors will be skipped with this try/ except\n",
    "    try:\n",
    "        #some of the links are video \"articles\" and I don't want to scrape those pages; there is very little information\n",
    "        if 'VIDEO' in article.text:\n",
    "            continue\n",
    "        #Take the URL extension, concatanate it with the base URL, and then add to the holder list\n",
    "        fox_urls.append(fox_base_url + article.find('a')['href'])\n",
    "            \n",
    "            \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "if len(fox_urls) == 0:\n",
    "    raise ValueError('Need to recheck the Fox code, not pulling URLs')\n",
    "\n",
    "#Loop across all URLs on the politics page to grab their article information\n",
    "for url in fox_urls:\n",
    "    \n",
    "    \n",
    "    #GET request of each page, grab the HTML text, and turn into BS object\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "\n",
    "    #Grab article title and append to holder list\n",
    "    current_title = soup.select(\"h1.headline\")[0].text\n",
    "    fox_titles.append(current_title)\n",
    "\n",
    "    #find the article section <p>s\n",
    "    article_text_sections = soup.select('#wrapper > div.page-content > div.row.full > main > article > div > div.article-content > div > p')\n",
    "\n",
    "    #grab all the paragraph element texts and join them together.    \n",
    "    current_article = ' '.join([p.text for p in article_text_sections])\n",
    "    #append the current article to the holder list\n",
    "    fox_text_of_articles.append(current_article)\n",
    "\n",
    "    #find the html section with the date\n",
    "    date_line = soup.select('#wrapper > div.page-content > div.row.full > main > article > header > div.article-meta.article-meta-upper > div.article-date > time')[0].text\n",
    "    #create regex element to rip the date out of that line\n",
    "    date_re = re.compile(r'\\w{3,}\\s\\d{1,2},\\s\\d{4}')\n",
    "    #find the date with the regex pattern\n",
    "    date = date_re.findall(date_line)[0]\n",
    "    #turn found date pattern into datetime element\n",
    "    date = datetime.strptime(date, '%B %d, %Y')\n",
    "    #turn date element into string in format \"MM/DD/YYYY\"\n",
    "    current_date = f\"{date.month}/{date.day}/{date.year}\"\n",
    "    #append date string to holder list\n",
    "    fox_date.append(current_date)\n",
    "\n",
    "#Create fox news dataframe to organize all collected information\n",
    "fox_df = pd.DataFrame(list(zip(fox_titles, fox_date, fox_urls, fox_text_of_articles)), columns = ['title','date','url', 'article_text'])\n",
    "\n",
    "#create new column with the source of these articles\n",
    "fox_df['source'] = 'Fox'\n",
    "\n",
    "#append the dataframe the new article CSV.\n",
    "fox_df.to_csv('news_articles.csv', index = False, header = False, mode= 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a04f5d07b0747026a8fbcdf50b9443318e69b1b8bd6247d88bfadb4789282972"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
